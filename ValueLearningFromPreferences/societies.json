{"ff": 
    {
        
        "groundings": {
                "default_professionalism": "ff_professionalism.npy",
                "professionalist": "ff_professionalist.npy",
                "default_proximity": "ff_proximity.npy",
                "proximitier": "ff_proximitier.npy"
                }
            ,
        "s1": {
            "name": "s1",
            "stochastic_expert": true,
            "same_trajectories_for_each_agent_type": true,
            "approx_expert": true,
            "policy_approximation_method": "mce_original",
            "_policy_approximation_method_options": ["mce_original", "new_value_iteration", "use_learner_class"],
            "approximator_kwargs": {"value_iteration_tolerance": 0.0000001, "iterations": 2000},
            "expert_policy_class": "PPO",
                "expert_kwargs" : {
                    "batch_size": 32,
                    "n_steps": 50,
                    "ent_coef": 0.1,
                    "learning_rate": 0.002,
                    "gamma": 1.0,
                    "gae_lambda": 0.999,
                    "clip_range": 0.1,
                    "vf_coef": 0.001,
                    "n_epochs": 10,
                    "normalize_advantage": true,
                    "tensorboard_log": "./ppo_tensorboard_expert_ff/"
                },
            "agents": [{
                "name": "normal",
                "grounding": ["default_professionalism", "default_proximity"], 
                "value_system": [0.5, 0.5],
                
                "n_agents": 4,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 1.0
                }
            },
            {
                "name": "proximity_only",
                "grounding": ["default_proximity", "default_proximity"], 
                "value_system": [1.0, 0.0],
                "n_agents": 4,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 1.0
                }
            },{
                "name": "prof_only",
                "grounding": ["default_professionalism", "default_professionalism"], 
                "value_system": [0.0, 1.0],
                "n_agents": 4,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 1.0
                }
            }]
        },
        "default": {
            "name": "default",
            "stochastic_expert": true,
            "same_trajectories_for_each_agent_type": true,
            "approx_expert": true,
            "policy_approximation_method": "mce_original",
            "_policy_approximation_method_options": ["mce_original", "new_value_iteration", "use_learner_class"],
            "approximator_kwargs": {"value_iteration_tolerance": 0.0000001, "iterations": 2000},
            "expert_policy_class": "PPO",
                "expert_kwargs" : {
                    "batch_size": 32,
                    "n_steps": 50,
                    "ent_coef": 0.1,
                    "learning_rate": 0.002,
                    "gamma": 1.0,
                    "gae_lambda": 0.999,
                    "clip_range": 0.1,
                    "vf_coef": 0.001,
                    "n_epochs": 10,
                    "normalize_advantage": true,
                    "tensorboard_log": "./ppo_tensorboard_expert_ff/"
                },
            "agents": [{
                "name": "proximity_agent",
                "grounding": ["default_professionalism", "default_proximity"], 
                "value_system": [0.0, 1.0],
                "n_agents": 5,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.5
                }
            }, {
                "name": "0208_agent",
                "grounding": ["default_professionalism", "default_proximity"], 
                "value_system": [0.2, 0.8],
                "n_agents": 5,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.5
                }
            }, {
                "name": "0406_agent",
                "grounding": ["default_professionalism", "default_proximity"], 
                "value_system": [0.4, 0.6],
                "n_agents": 5,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.5
                }
            }, {
                "name": "0604_agent",
                "grounding": ["default_professionalism", "default_proximity"], 
                "value_system": [0.6, 0.4],
                "n_agents": 5,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.5
                }
            }, {
                "name": "0802_agent",
                "grounding": ["default_professionalism", "default_proximity"], 
                "value_system": [0.8, 0.2],
                "n_agents": 5,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.5
                    
                }
            },{
                "name": "prof_agent",
                "grounding": ["default_professionalism", "default_proximity"], 
                "value_system": [1.0, 0.0],
                "n_agents": 5,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.5
                }
            }]
        }
    },
"rw": 
    {
        
        "groundings": {
                "default_sus": "rw_sustainability.npy",
                "default_com": "rw_comfort.npy",
                "default_eff": "rw_efficiency.npy"
                }
            ,
        "default": {
            "name": "s1",
            "stochastic_expert": false,
            "same_trajectories_for_each_agent_type": true,
            "approx_expert": true,
            "policy_approximation_method": "new_value_iteration",
            "_policy_approximation_method_options": ["mce_original", "new_value_iteration", "use_learner_class"],
            "approximator_kwargs": {"value_iteration_tolerance": 0.0000001, "iterations": 2000},
            "expert_policy_class": "PPO",
                "expert_kwargs" : {
                    "batch_size": 32,
                    "n_steps": 50,
                    "ent_coef": 0.1,
                    "learning_rate": 0.002,
                    "gamma": 1.0,
                    "gae_lambda": 0.999,
                    "clip_range": 0.1,
                    "vf_coef": 0.001,
                    "n_epochs": 10,
                    "normalize_advantage": true,
                    "tensorboard_log": "./ppo_tensorboard_expert_rw/"
                },
            "agents": [{
                "name": "100agent",
                "grounding": ["default_sus", "default_com", "default_eff"], 
                "value_system": [1.0, 0.0, 0.0],
                "n_agents": 4,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.0
                }
            },
            {
                "name": "010agent",
                "grounding": ["default_sus", "default_com", "default_eff"], 
                "value_system": [0.0, 1.0, 0.0],
                "n_agents": 4,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.0
                }
            },
            {
                "name": "001agent",
                "grounding": ["default_sus", "default_com", "default_eff"], 
                "value_system": [0.0, 0.0, 1.0],
                "n_agents": 4,
                "data": {
                    "trajectory_pairs": 50,
                    "rationality": 1.0, "random_traj_proportion": 0.0
                }
            }]
        }
    },
    "apollo": {
        "groundings": {
                "default_eff": "ap_travel_time.npy",
                "default_ach": "ap_travel_cost.npy",
                "default_sec": "ap_headway.npy",
                "default_comf": "ap_transfers.npy"
                },

        "default": {
            "name": "default",
            "same_trajectories_for_each_agent_type": false,
            "stochastic_expert": true
    }
}
}